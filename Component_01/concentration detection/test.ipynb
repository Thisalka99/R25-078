{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe Face Mesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    static_image_mode=False,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "# Indices for key landmarks on the iris and eye corners\n",
    "left_iris_index = 468\n",
    "left_eye_inner = 133\n",
    "left_eye_outer = 33\n",
    "left_eye_top = 159\n",
    "left_eye_bottom = 145\n",
    "\n",
    "right_iris_index = 473\n",
    "right_eye_inner = 362\n",
    "right_eye_outer = 263\n",
    "right_eye_top = 386\n",
    "right_eye_bottom = 374\n",
    "\n",
    "    \n",
    "# Function to calculate gaze direction\n",
    "def is_concentrated(face_landmarks, frame_width, frame_height):\n",
    "    \n",
    "    # Get positions of left and right iris centers\n",
    "   \n",
    "    left_rye_z = face_landmarks.landmark[left_iris_index].z\n",
    "    right_iris_z = face_landmarks.landmark[right_iris_index].z\n",
    "    \n",
    "    left_eye_y = face_landmarks.landmark[left_eye_outer].y * frame_height\n",
    "    right_eye_y = face_landmarks.landmark[right_eye_outer].y * frame_height\n",
    "    eye_y_difference = abs(left_eye_y - right_eye_y)\n",
    "    \n",
    "    # Tilted face\n",
    "    if eye_y_difference > 15: # higher = more angle\n",
    "        return False\n",
    "    \n",
    "    left_iris_x = face_landmarks.landmark[left_iris_index].x * frame_width\n",
    "    right_iris_x = face_landmarks.landmark[right_iris_index].x * frame_width\n",
    "\n",
    "    # Get eye corner positions\n",
    "    left_eye_inner_x = face_landmarks.landmark[left_eye_inner].x * frame_width\n",
    "    left_eye_outer_x = face_landmarks.landmark[left_eye_outer].x * frame_width\n",
    "    right_eye_inner_x = face_landmarks.landmark[right_eye_inner].x * frame_width\n",
    "    right_eye_outer_x = face_landmarks.landmark[right_eye_outer].x * frame_width\n",
    "\n",
    "    # Calculate relative positions for gaze detection\n",
    "    left_gaze_ratio = (left_iris_x - left_eye_inner_x) / (left_eye_outer_x - left_eye_inner_x)\n",
    "    right_gaze_ratio = (right_iris_x - right_eye_inner_x) / (right_eye_outer_x - right_eye_inner_x)\n",
    "    \n",
    "    z_diff = abs(left_rye_z - right_iris_z)\n",
    "    \n",
    "    # Check the head pose\n",
    "    if z_diff > 0.028: # head pose (higher = more angle)\n",
    "        \n",
    "        if (left_rye_z > right_iris_z): # looking left\n",
    "\n",
    "            if  left_gaze_ratio > 0.55: # higher = more angle\n",
    "                 return False\n",
    "            else:\n",
    "              return True\n",
    "            \n",
    "        else: # looking right\n",
    "            if  right_gaze_ratio > 0.55: # higher = more angle\n",
    "                return False\n",
    "            else:\n",
    "              return True\n",
    "            \n",
    "       \n",
    "    # Check if both irises are centered within each eye\n",
    "    if abs(left_gaze_ratio - right_gaze_ratio) < 0.14: # higher = more angle\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "# Start video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Mirror effect for better visualization\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(frame_rgb)\n",
    "\n",
    "    # If landmarks are detected, calculate gaze and head position\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            # Calculate gaze direction and head tilt\n",
    "            concentrated = is_concentrated(face_landmarks, frame_width, frame_height)\n",
    "            \n",
    "            if concentrated:\n",
    "                text = \"Concentrated\"\n",
    "                color = (0, 255, 0)\n",
    "            else:\n",
    "                text = \"Not Concentrated\"\n",
    "                color = (0, 0, 255)\n",
    "\n",
    "            # Display the gaze direction and head tilt on the frame\n",
    "            cv2.putText(frame, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)      \n",
    "\n",
    "    # Show the frame\n",
    "    cv2.imshow('Eyeball & Head Pose Detection', frame)\n",
    "\n",
    "    # Exit on pressing 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "face_mesh.close()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
